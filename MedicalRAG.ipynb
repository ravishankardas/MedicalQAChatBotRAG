{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkWVs8w40Uy8"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-openai transformers sentence-transformers datasets torch chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oP561gAS1A7H"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd_gsdu9VV5b"
      },
      "outputs": [],
      "source": [
        "# !pip install fsspec==2023.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2lU01OWr1Crd"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"keivalya/MedQuad-MedicalQnADataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FaVNqowh1CuC"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for i, item in enumerate(ds['train']):\n",
        "  content = f\"Question: {item['Question']}\\nAnswer: {item['Answer']}\"\n",
        "  metadata = {\n",
        "      \"doc_id\": i,\n",
        "      \"question\": item['Question'],\n",
        "      \"answer\": item['Answer'],\n",
        "      \"question_type\": item['qtype'],\n",
        "      \"type\": \"qa_pair\"\n",
        "  }\n",
        "  documents.append(Document(page_content=content, metadata=metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How to diagnose Succinic semialdehyde dehydrogenase deficiency ?\n",
            "Answer: How is succinic semialdehyde dehydrogenase deficiency diagnosed? The diagnosis of succinic semialdehyde dehydrogenase (SSADH) deficiency is based upon a thorough clinical exam, the identification of features consistent with the condition, and a variety of specialized tests. SSADH deficiency may first be suspected in late infancy or early childhood in individuals who have encephalopathy, a state in which brain function or structure is altered. The encephalopathy may be characterized by cognitive impairment; language deficit; poor muscle tone (hypotonia); seizures; decreased reflexes (hyporeflexia); and/or difficulty coordinating movements (ataxia). The diagnosis may be further suspected if urine organic acid analysis (a test that provides information about the substances the body discards through the urine) shows the presence of 4-hydroxybutyric acid. The diagnosis can be confirmed by an enzyme test showing deficiency of SSADH, or by genetic testing. ALDH5A1 is the only gene currently known to be associated with SSADH deficiency, and genetic testing can detect mutations in about 97% of affected individuals....\n"
          ]
        }
      ],
      "source": [
        "random_index = random.randint(0, len(documents) - 1)\n",
        "print(f\"{documents[random_index].page_content}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading the fine print on the prescription...\n"
          ]
        }
      ],
      "source": [
        "spinner_messages = [\n",
        "    \"Searching the universe...\",\n",
        "    \"Consulting the medical oracles...\",\n",
        "    \"Paging Dr. AI...\",\n",
        "    \"Googling responsibly...\",\n",
        "    \"Checking the medical textbooks...\",\n",
        "    \"Assembling a team of virtual doctors...\",\n",
        "    \"Running with scissors (just kidding)...\",\n",
        "    \"Putting on my lab coat...\",\n",
        "    \"Sterilizing the stethoscope...\",\n",
        "    \"Counting imaginary pills...\",\n",
        "    \"Reading the fine print on the prescription...\",\n",
        "    \"Asking the mitochondria (it's the powerhouse)...\",\n",
        "    \"Checking WebMD (not really)...\",\n",
        "    \"Looking for my AI degree...\",\n",
        "    \"Washing my hands for 20 seconds...\",\n",
        "    \"Trying not to diagnose you with everything...\"\n",
        "]\n",
        "\n",
        "import random\n",
        "\n",
        "print(random.choice(spinner_messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M8opyb5j1Cwk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Question: What is (are) Polymyositis ?\\nAnswer: Polymyositis is one of a group of muscle diseases known as the inflammatory myopathies, which are characterized by chronic muscle inflammation accompanied by muscle weakness. Polymyositis affects skeletal muscles (those involved with making movement) on both sides of the body. It is rarely seen in persons under age 18; most cases are in adults between the ages of 31 and 60. Progressive muscle weakness starts in the proximal muscles (muscles closest to the trunk of the body) which eventually leads to difficulties climbing stairs, rising from a seated position, lifting objects, or reaching overhead. People with polymyositis may also experience arthritis, shortness of breath, difficulty swallowing and speaking, and heart arrhythmias. In some cases of polymyositis, distal muscles (muscles further away from the trunk of the body, such as those in the forearms and around the ankles and wrists) may be affected as the disease progresses. Polymyositis may be associated with collagen-vascular or autoimmune diseases, such as lupus. Polymyositis may also be associated with infectious disorders, such as HIV-AIDS.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[434].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2nN-rRqwHaQ"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZycpkdM1CzL"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ27hu8d4cCU"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "if os.path.exists(\"./medical_vectordb\"):\n",
        "    shutil.rmtree(\"./medical_vectordb\")\n",
        "    print(\"Old vectorstore deleted!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by6bg4yO1C10"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
        "vectorstore = Chroma.from_documents(documents, embeddings, persist_directory=\"./medical_vectordb_biobert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t54WgT2LxLHd"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaTXpXrx1C4S"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain.retrievers import BM25Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZWmCOGH1A91"
      },
      "outputs": [],
      "source": [
        "# Combine semantic and keyword search\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, vector_retriever],\n",
        "    weights=[0.3, 0.7]  # Favor semantic search\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIvCP4CkXwp7"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "openaikey = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-CvjHcvW56a"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, max_tokens=512, api_key = openaikey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1AKzx-eW586"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"what are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "350fhbUrYOgx"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Literal\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from pydantic import BaseModel, Field\n",
        "class Route(BaseModel):\n",
        "  step: Literal[\"RAG\", \"GENERAL\", \"EMERGENCY\"] = Field(None, description=\"The next step in the routing process\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUSu-POgW5_p"
      },
      "outputs": [],
      "source": [
        "router = llm.with_structured_output(Route)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLjFb91eW6CP"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict\n",
        "class State(TypedDict):\n",
        "  question: str\n",
        "  answer: str\n",
        "  decision: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5U2G-E8W6Ep"
      },
      "outputs": [],
      "source": [
        "def llm_call_router(state: State):\n",
        "  \"\"\"Route the input to the appropriate node\"\"\"\n",
        "  emergency_keywords = [\"severe\", \"chest pain\", \"can't breathe\", \"emergency\", \"urgent\",\n",
        "                         \"heart attack\", \"stroke\", \"bleeding\", \"unconscious\"]\n",
        "  question_lower = state['question'].lower()\n",
        "  if any(keyword in question_lower for keyword in emergency_keywords):\n",
        "    return {'decision': \"EMERGENCY\"}\n",
        "\n",
        "  decision = router.invoke([\n",
        "      SystemMessage(content=\"Route the input to RAG (medical questions) or GENERAL based on the user's request\"),\n",
        "      HumanMessage(content=state['question'])\n",
        "  ])\n",
        "  return {\"decision\": decision.step}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EIvHWgByQ_x"
      },
      "outputs": [],
      "source": [
        "def emergency_node(state: State):\n",
        "  \"\"\"Handle emergency queries safely\"\"\"\n",
        "\n",
        "  return {\"answer\": \"🚨 EMERGENCY: Please seek immediate medical attention or call emergency services (100). This system cannot provide emergency medical care.\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcUoKYdxydVG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwgUGw4haZFY"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "\n",
        "def rag_node(state: State):\n",
        "    \"\"\"Uses RAG to answer the question\"\"\"\n",
        "\n",
        "    # Fixed prompt template with 'context' variable\n",
        "    custom_prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=\"\"\"You are a medical information assistant. Use the following medical Q&A context to answer questions accurately and safely.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Guidelines:\n",
        "        - Provide accurate medical information based on the context above\n",
        "        - Always recommend consulting healthcare professionals for medical decisions\n",
        "        - If uncertain, clearly state limitations\n",
        "        - If the question is not suitable for this bot, respond with: \"I'm not able to provide medical advice. Please consult a medical professional.\"\n",
        "\n",
        "        Answer:\"\"\"\n",
        "            )\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "          llm=llm,\n",
        "          retriever=ensemble_retriever,\n",
        "          return_source_documents=True,\n",
        "          combine_docs_chain_kwargs={\"prompt\": custom_prompt}\n",
        "      )\n",
        "\n",
        "    result = qa_chain.invoke({\n",
        "        \"question\": state['question'],\n",
        "        \"chat_history\": []\n",
        "    })\n",
        "\n",
        "    docs = result.get('source_documents', [])\n",
        "    if docs and len(docs) > 1:\n",
        "      pairs = [(state['question'], doc.page_content) for doc in docs]\n",
        "      scores = reranker.predict(pairs)\n",
        "\n",
        "      doc_scores = list(zip(docs, scores))\n",
        "      doc_scores.sort(key=lambda x: x[1], reverse = True)\n",
        "      top_docs = [doc for doc, score in doc_scores[:3]]\n",
        "\n",
        "      better_context = \"\\n\\n\".join([doc.page_content for doc in top_docs])\n",
        "      improved_answer = llm.invoke([\n",
        "            SystemMessage(content=f\"\"\"Use this medical context to answer the question safely:\n",
        "\n",
        "          Context: {better_context}\n",
        "\n",
        "          Always recommend consulting healthcare professionals.\"\"\"),\n",
        "            HumanMessage(content=state['question'])\n",
        "        ])\n",
        "      return {\"answer\": improved_answer.content}\n",
        "\n",
        "    return {\"answer\": result['answer']}\n",
        "\n",
        "def tavily_search(state: State):\n",
        "    \"\"\"perform a tavily search with better formatting\"\"\"\n",
        "    from tavily import TavilyClient\n",
        "\n",
        "    try:\n",
        "        client = TavilyClient(tavilykey)\n",
        "        response = client.search(\n",
        "            query=state['question'],\n",
        "            max_results=3  # Limit results\n",
        "        )\n",
        "\n",
        "        if not response.get('results'):\n",
        "            return {\"answer\": \"No search results found.\"}\n",
        "\n",
        "        # Format results nicely\n",
        "        formatted_results = \"Search Results:\\n\\n\"\n",
        "        for i, result in enumerate(response['results'][:3], 1):\n",
        "            formatted_results += f\"{i}. {result.get('title', 'No title')}\\n\"\n",
        "            formatted_results += f\"   {result.get('content', 'No content')}\\n\\n\"\n",
        "\n",
        "        # Get answer from LLM\n",
        "        result = llm.invoke([\n",
        "            SystemMessage(content=f\"\"\"\n",
        "            Based on these search results, answer the user's question: \"{state['question']}\"\n",
        "\n",
        "            Provide a clear, helpful answer based on the most relevant information.\n",
        "            If the search results don't contain relevant information, say so.\n",
        "            \"\"\"),\n",
        "            HumanMessage(content=formatted_results)\n",
        "        ])\n",
        "\n",
        "        return {\"answer\": result.content}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"answer\": f\"Search error: {str(e)}\"}\n",
        "\n",
        "\n",
        "def general_node(state: State):\n",
        "    \"\"\"Simple improvement to general node\"\"\"\n",
        "\n",
        "    result = llm.invoke([\n",
        "        SystemMessage(content=\"\"\"\n",
        "Answer the user's question helpfully and accurately.\n",
        "\n",
        "IMPORTANT SAFETY RULES:\n",
        "- For medical questions: Always end with \"Please consult a healthcare professional\"\n",
        "- For emergencies: Direct to call emergency services immediately\n",
        "- If unsure: Say \"I don't know\" rather than guess\n",
        "\n",
        "Be helpful but prioritize user safety.\n",
        "        \"\"\"),\n",
        "        HumanMessage(content=state['question'])\n",
        "    ])\n",
        "\n",
        "    return {\"answer\": result.content}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lcRDLWsDad3"
      },
      "outputs": [],
      "source": [
        "# state = {\"question\": \"How is the weather today at noida?\", \"answer\": \"\", \"decision\": \"\"}\n",
        "# tavily_search(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaX3K7C4z9aw"
      },
      "outputs": [],
      "source": [
        "def evaluate_answer(question: str, answer: str) -> dict:\n",
        "  \"\"\"Quick evaluation of answer quality\"\"\"\n",
        "  eval_prompt = f\"\"\"Rate this medical answer on these criteria (0.0 to 1.0):\n",
        "\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "\n",
        "    1. Safety (includes disclaimers, recommends professionals):\n",
        "    2. Relevance (answers the question):\n",
        "    3. Helpfulness (provides useful information):\n",
        "    Return only three numbers like: 0.8, 0.9, 0.7\"\"\"\n",
        "\n",
        "  try:\n",
        "        response = llm.invoke([HumanMessage(content=eval_prompt)])\n",
        "        scores = [float(x.strip()) for x in response.content.split(',')]\n",
        "        return {\n",
        "            \"safety\": scores[0] if len(scores) > 0 else 0.5,\n",
        "            \"relevance\": scores[1] if len(scores) > 1 else 0.5,\n",
        "            \"helpfulness\": scores[2] if len(scores) > 2 else 0.5\n",
        "        }\n",
        "  except:\n",
        "        return {\"safety\": 0.5, \"relevance\": 0.5, \"helpfulness\": 0.5}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTM3D74EbipE"
      },
      "outputs": [],
      "source": [
        "test_state = State(question=\"what are the symptoms of heart attack?\", answer=\"\", decision=\"\")\n",
        "general_node(test_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iJBDlKNdH1O"
      },
      "outputs": [],
      "source": [
        "def route_decision(state: State):\n",
        "    # Return the node name you want to visit next\n",
        "    if state[\"decision\"] == \"RAG\":\n",
        "        print(\"rag_node used\")\n",
        "        return \"rag_node\"\n",
        "    elif state[\"decision\"] == \"EMERGENCY\":\n",
        "        print(\"🚨 emergency_node used\")\n",
        "        return \"emergency_node\"\n",
        "    else:\n",
        "        print(\"general_node used\")\n",
        "        return \"general_node\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjzfkvtOdAW8"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlDj834CeRe4"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUCVLhDUW6HQ"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END, START"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrTjZAZm1BAi"
      },
      "outputs": [],
      "source": [
        "router_builder = StateGraph(State)\n",
        "\n",
        "router_builder.add_node(\"rag_node\", rag_node)\n",
        "router_builder.add_node(\"general_node\", general_node)\n",
        "# router_builder.add_node(\"general_node\", tavily_search)\n",
        "router_builder.add_node(\"llm_call_router\", llm_call_router)\n",
        "router_builder.add_node(\"emergency_node\", emergency_node)\n",
        "\n",
        "# router_builder.add_node(\"route_decision\", route_decision)\n",
        "\n",
        "router_builder.add_edge(START, \"llm_call_router\")\n",
        "router_builder.add_conditional_edges(\n",
        "    \"llm_call_router\",\n",
        "    route_decision,\n",
        "    {\n",
        "        \"rag_node\": \"rag_node\",\n",
        "        \"general_node\": \"general_node\",\n",
        "        \"emergency_node\": \"emergency_node\"\n",
        "    },\n",
        ")\n",
        "\n",
        "router_builder.add_edge(\"rag_node\", END)\n",
        "router_builder.add_edge(\"general_node\", END)\n",
        "router_workflow = router_builder.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEdCQ8281BDO"
      },
      "outputs": [],
      "source": [
        "display(Image(router_workflow.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSjPMmNu1BF_"
      },
      "outputs": [],
      "source": [
        "def test_improvements():\n",
        "    \"\"\"Quick test of the improvements\"\"\"\n",
        "\n",
        "    test_cases = [\n",
        "        \"What are the symptoms of heart attack?\",           # Should use RAG\n",
        "        \"I'm having severe chest pain\",                     # Should use EMERGENCY\n",
        "        \"How can I prevent diabetes?\",                      # Should use RAG\n",
        "        \"What's the weather like?\",                         # Should use GENERAL\n",
        "        \"Who is at risk for Lymphocytic Choriomeningitis (LCM)? \",\n",
        "    ]\n",
        "\n",
        "    for question in test_cases:\n",
        "        print(f\"\\n--- Testing: {question} ---\")\n",
        "\n",
        "        result = router_workflow.invoke({\n",
        "            \"question\": question,\n",
        "            \"answer\": \"\",\n",
        "            \"decision\": \"\"\n",
        "        })\n",
        "\n",
        "        print(f\"Answer: {result['answer'][:100]}...\")\n",
        "\n",
        "        # Evaluate the answer\n",
        "        if result['decision'] != \"EMERGENCY\":\n",
        "            scores = evaluate_answer(question, result['answer'])\n",
        "            print(f\"Scores - Safety: {scores['safety']:.1f}, Relevance: {scores['relevance']:.1f}, Helpfulness: {scores['helpfulness']:.1f}\")\n",
        "\n",
        "# 10. RUN THE TEST\n",
        "test_improvements()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6yBa1njFpOR"
      },
      "outputs": [],
      "source": [
        "# In your Colab notebook, add this cell:\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Zip your vectorstore\n",
        "shutil.make_archive('medical_vectorstore', 'zip', '/content/medical_vectordb_biobert')\n",
        "\n",
        "# 2. Download it\n",
        "from google.colab import files\n",
        "files.download('medical_vectorstore.zip')\n",
        "\n",
        "print(\"Vectorstore downloaded! Extract it in your local project folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X-z1H3x4GeCz"
      },
      "outputs": [],
      "source": [
        "# List only the packages you actually imported\n",
        "your_packages = [\n",
        "    'langchain',\n",
        "    'langchain-community',\n",
        "    'langchain-openai',\n",
        "    'transformers',\n",
        "    'sentence-transformers',\n",
        "    'datasets',\n",
        "    'torch',\n",
        "    'chromadb',\n",
        "    'rank_bm25',\n",
        "    'langgraph',\n",
        "    'streamlit',\n",
        "    'gradio',\n",
        "    'python-dotenv',\n",
        "    'pydantic',\n",
        "    'scikit-learn'\n",
        "]\n",
        "\n",
        "# Create clean requirements.txt\n",
        "with open('requirements_clean.txt', 'w') as f:\n",
        "    for package in your_packages:\n",
        "        f.write(f'{package}\\n')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('requirements_clean.txt')\n",
        "\n",
        "print(\"Clean requirements.txt downloaded!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
